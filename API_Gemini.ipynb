{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2de7dcb",
   "metadata": {},
   "source": [
    "**ABSTRACT:**\n",
    "\n",
    "This is a Proof of Concept (PoC) of how to connect to an API of a commercial LLM. \n",
    "\n",
    "Also, we measured how long one or more queries take which each of the available models.  \n",
    "\n",
    "The values below show that API calls take most of the duration of a query. One (1) query or a batch of five (5) take similar duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4679fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5e7ab",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemini-api/docs/models#model-variations contains the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e384d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "client = genai.Client(api_key=api_key)\n",
    "model_list = [\"gemini-2.5-pro\", \"gemini-2.5-flash\", \"gemini-2.5-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213547c",
   "metadata": {},
   "source": [
    "There are limitated queries per minute: https://ai.google.dev/gemini-api/docs/rate-limits  \n",
    "Gemini 2.5 Pro limit is 5 queries per minute -> you cannot run this notebook in one minute, you need to run it cell by cell  \n",
    "Command to get info from a model: model_info = client.models.get(model=\"gemini-2.0-flash\"). Most of this info is found in the first link  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e5fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model gemini-2.5-pro gave the response: **In 5 words:**\n",
      "\n",
      "Computers learning from vast data.\n",
      "\n",
      "---\n",
      "\n",
      "**In one sentence:**\n",
      "\n",
      "AI finds patterns in data to make predictions, decisions, or create new things.\n",
      "\n",
      "---\n",
      "\n",
      "**As an analogy:**\n",
      "\n",
      "Imagine teaching a child by showing it millions of pictures of a cat. Eventually, it learns to recognize a cat on its own. AI does the same with data.\n",
      "The model gemini-2.5-flash gave the response: AI learns from data to recognize patterns and make decisions.\n",
      "The model gemini-2.5-flash-lite gave the response: AI learns from data to make decisions or predictions.\n",
      "The model gemini-2.0-flash gave the response: AI learns patterns from data to make predictions or decisions.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave the response: AI learns and reasons like humans.\n",
      "\n",
      "The model gemini-2.5-pro took  15.0 seconds to respond\n",
      "The model gemini-2.5-flash took  5.5 seconds to respond\n",
      "The model gemini-2.5-flash-lite took  0.5 seconds to respond\n",
      "The model gemini-2.0-flash took  0.5 seconds to respond\n",
      "The model gemini-2.0-flash-lite took  0.3 seconds to respond\n"
     ]
    }
   ],
   "source": [
    "# One query\n",
    "responses = {}\n",
    "durations = {}\n",
    "for model in model_list:\n",
    "    start_time = time.time()\n",
    "    response = client.models.generate_content(model=model, contents=\"Explain how AI works in a few words\")\n",
    "    responses.update({model: response.text})\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    durations.update({model: duration})\n",
    "\n",
    "for key, value in responses.items():\n",
    "    print(f'The model {key} gave the response: {value}')\n",
    "\n",
    "for key, value in durations.items():\n",
    "    print(f'The model {key} took {value: .1f} seconds to respond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf63c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model gemini-2.5-pro gave to the query number 1 the response: AI learns from data to **recognize patterns** and **make predictions**.\n",
      "\n",
      "***\n",
      "\n",
      "**Even simpler:**\n",
      "Finding patterns in data to make decisions.\n",
      "\n",
      "**As an analogy:**\n",
      "It's like teaching a computer by showing it millions of examples, so it can learn a skill on its own.\n",
      "The model gemini-2.5-pro gave to the query number 2 the response: **Finding patterns in data to make predictions.**\n",
      "\n",
      "---\n",
      "\n",
      "**A little more:**\n",
      "\n",
      "Instead of being explicitly programmed with rules, a machine **learns the rules itself** by analyzing thousands of examples.\n",
      "The model gemini-2.5-pro gave to the query number 3 the response: **Collect** huge, varied data sets, **analyze** them for hidden patterns, and **use** the insights to make smarter decisions.\n",
      "The model gemini-2.5-pro gave to the query number 4 the response: You pay a monthly fee to instantly stream a huge library of movies and TV shows over the internet.\n",
      "The model gemini-2.5-pro gave to the query number 5 the response: **They buy food in bulk, display it on shelves, and sell it to customers for a profit.**\n",
      "The model gemini-2.5-flash gave to the query number 1 the response: AI learns patterns from data to make predictions or decisions.\n",
      "The model gemini-2.5-flash gave to the query number 2 the response: ML teaches computers to learn from data to find patterns and make predictions, improving over time.\n",
      "The model gemini-2.5-flash gave to the query number 3 the response: Big Data collects and analyzes **massive, complex datasets** to find **hidden patterns and insights**, enabling **smarter decisions and predictions**.\n",
      "The model gemini-2.5-flash gave to the query number 4 the response: Netflix allows subscribers to **stream a vast library of movies and TV shows on-demand over the internet** for a monthly fee.\n",
      "The model gemini-2.5-flash gave to the query number 5 the response: A supermarket buys food and goods in bulk from suppliers and sells them directly to consumers for a profit.\n",
      "The model gemini-2.5-flash-lite gave to the query number 1 the response: AI learns from data to make decisions and predictions.\n",
      "The model gemini-2.5-flash-lite gave to the query number 2 the response: ML learns from data to make predictions or decisions without explicit programming.\n",
      "The model gemini-2.5-flash-lite gave to the query number 3 the response: Big Data analyzes vast, complex datasets to reveal patterns and insights.\n",
      "The model gemini-2.5-flash-lite gave to the query number 4 the response: Netflix streams movies and shows over the internet.\n",
      "The model gemini-2.5-flash-lite gave to the query number 5 the response: Sells food and household items, acting as a central marketplace.\n",
      "The model gemini-2.0-flash gave to the query number 1 the response: AI learns patterns from data to make predictions or decisions.\n",
      "\n",
      "The model gemini-2.0-flash gave to the query number 2 the response: ML learns patterns from data to make predictions or decisions.\n",
      "\n",
      "The model gemini-2.0-flash gave to the query number 3 the response: Analyzing massive, complex datasets for insights and patterns.\n",
      "\n",
      "The model gemini-2.0-flash gave to the query number 4 the response: Netflix streams movies and TV shows over the internet on demand.\n",
      "\n",
      "The model gemini-2.0-flash gave to the query number 5 the response: A supermarket buys goods in bulk, displays them attractively, and sells them to customers for a profit.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave to the query number 1 the response: AI learns and makes decisions, like humans.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave to the query number 2 the response: **Learn from data, make predictions.**\n",
      "\n",
      "The model gemini-2.0-flash-lite gave to the query number 3 the response: Collect, store, analyze, repeat.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave to the query number 4 the response: Netflix streams movies and shows on demand via the internet.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave to the query number 5 the response: Sells food & goods for profit.\n",
      "\n",
      "The model gemini-2.5-pro took  75.7 seconds to respond, at an average rate of  15.1 seconds per query\n",
      "The model gemini-2.5-flash took  24.5 seconds to respond, at an average rate of  4.9 seconds per query\n",
      "The model gemini-2.5-flash-lite took  2.5 seconds to respond, at an average rate of  0.5 seconds per query\n",
      "The model gemini-2.0-flash took  2.7 seconds to respond, at an average rate of  0.5 seconds per query\n",
      "The model gemini-2.0-flash-lite took  1.8 seconds to respond, at an average rate of  0.4 seconds per query\n"
     ]
    }
   ],
   "source": [
    "# 5 API call with 5 queries\n",
    "responses = {}\n",
    "durations = {}\n",
    "for model in model_list:\n",
    "    start_time = time.time()\n",
    "    query_number = 0\n",
    "    queries_responded = []\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=\"Explain how AI works in a few words\"\n",
    "    )\n",
    "    query_number += 1\n",
    "    queries_responded.append({query_number: response.text})\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=\"Explain how ML works in a few words\"\n",
    "    )\n",
    "    query_number += 1\n",
    "    queries_responded.append({query_number: response.text})\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=\"Explain how Big Data works in a few words\"\n",
    "    )\n",
    "    query_number += 1\n",
    "    queries_responded.append({query_number: response.text})\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=\"Explain how Netfilx works in a few words\"\n",
    "    )\n",
    "    query_number += 1\n",
    "    queries_responded.append({query_number: response.text})\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=\"Explain how a Supermarket or grocery store works in a few words\"\n",
    "    )\n",
    "    query_number += 1\n",
    "    queries_responded.append({query_number: response.text})\n",
    "\n",
    "    responses.update({model: queries_responded})\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    durations.update({model: duration})\n",
    "\n",
    "for model, queries_responded in responses.items():\n",
    "    for pairs in queries_responded:\n",
    "        for key, response in pairs.items():\n",
    "            print(f'The model {model} gave to the query number {key} the response: {response}')\n",
    "\n",
    "for key, value in durations.items():\n",
    "    print(f'The model {key} took {value: .1f} seconds to respond, at an average rate of {value/5: .1f} seconds per query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e243800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model gemini-2.5-pro gave the response: **AI (Artificial Intelligence)**\n",
      "Computers learn, reason, and solve problems like humans.\n",
      "\n",
      "**ML (Machine Learning)**\n",
      "Learning from patterns in data, not explicit instructions.\n",
      "\n",
      "**Big Data**\n",
      "Analyzing massive, complex data to uncover hidden patterns.\n",
      "\n",
      "**Netflix**\n",
      "Streams content online, using your data to recommend what to watch next.\n",
      "\n",
      "**Supermarket**\n",
      "Buys products in bulk, sells them individually to customers for profit.\n",
      "The model gemini-2.5-flash gave the response: Here's how each works, in a few words:\n",
      "\n",
      "*   **AI (Artificial Intelligence):** Teaching computers to think and learn like humans to solve problems.\n",
      "*   **ML (Machine Learning):** Algorithms learn patterns from data to make predictions and decisions, improving over time.\n",
      "*   **Big Data:** Massive, complex datasets analyzed to reveal hidden patterns, trends, and insights.\n",
      "*   **Netflix:** A subscription service offering on-demand streaming of a vast library of movies and TV shows over the internet.\n",
      "*   **Supermarket/Grocery Store:** A large retail store where customers self-select a wide variety of food and household goods, then purchase them at a checkout.\n",
      "The model gemini-2.5-flash-lite gave the response: Here's a breakdown of each in a few words:\n",
      "\n",
      "*   **AI:** Machines mimicking human intelligence and decision-making.\n",
      "*   **ML:** Machines learning from data without explicit programming.\n",
      "*   **Big Data:** Vast, complex datasets analyzed for insights.\n",
      "*   **Netflix:** Streaming movies/shows based on viewing habits.\n",
      "*   **Supermarket:** Selling a wide variety of food and household goods.\n",
      "The model gemini-2.0-flash gave the response: Okay, here's a breakdown in a few words each:\n",
      "\n",
      "*   **AI:** Machines mimicking intelligent human behavior.\n",
      "*   **ML:** Learning patterns from data to predict.\n",
      "*   **Big Data:** Analyzing massive datasets for insights.\n",
      "*   **Netflix:** Streaming personalized video on demand.\n",
      "*   **Supermarket:** Selling diverse groceries for profit.\n",
      "\n",
      "The model gemini-2.0-flash-lite gave the response: Here's a breakdown in concise terms:\n",
      "\n",
      "*   **AI:** Mimicking human intelligence with machines.\n",
      "*   **ML:** Learning from data to improve performance.\n",
      "*   **Big Data:** Massive datasets, analyzed for insights.\n",
      "*   **Netflix:** Streaming content, personalized recommendations.\n",
      "*   **Supermarket:** Buying and selling goods, supply chain.\n",
      "\n",
      "The model gemini-2.5-pro took  18.0 seconds to respond, at an average rate of  3.6 seconds per query\n",
      "The model gemini-2.5-flash took  6.8 seconds to respond, at an average rate of  1.4 seconds per query\n",
      "The model gemini-2.5-flash-lite took  0.6 seconds to respond, at an average rate of  0.1 seconds per query\n",
      "The model gemini-2.0-flash took  1.1 seconds to respond, at an average rate of  0.2 seconds per query\n",
      "The model gemini-2.0-flash-lite took  0.8 seconds to respond, at an average rate of  0.2 seconds per query\n"
     ]
    }
   ],
   "source": [
    "# 1 API call with 5 queries\n",
    "responses = {}\n",
    "durations = {}\n",
    "for model in model_list:\n",
    "    start_time = time.time()\n",
    "    response = client.models.generate_content(\n",
    "        model=model, contents=[\n",
    "            \"Explain how AI works in a few words\", \n",
    "            \"Explain how ML works in a few words\",\n",
    "            \"Explain how Big Data works in a few words\",\n",
    "            \"Explain how Netfilx works in a few words\",\n",
    "            \"Explain how a Supermarket or grocery store works in a few words\"\n",
    "            ] \n",
    "    )\n",
    "    responses.update({model: response.text})\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    durations.update({model: duration})\n",
    "\n",
    "for key, value in responses.items():\n",
    "    print(f'The model {key} gave the response: {value}')\n",
    "\n",
    "for key, value in durations.items():\n",
    "    print(f'The model {key} took {value: .1f} seconds to respond, at an average rate of {value/5: .1f} seconds per query')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
